# Complete Pipeline Guide - Baseline Model

## íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ëª…ë ¹ì–´

### 0. í™˜ê²½ ì¤€ë¹„ (ìµœì´ˆ 1íšŒ)

```bash
cd /Users/yoonseo/Desktop/data304_final

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source data304/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜ í™•ì¸
pip install -r requirements.txt
```

---

## ğŸ”¹ STEP 1: Silver Label ìƒì„± (Hybrid Top-Down)

### ëª…ë ¹ì–´
```bash
python3 src/silver_labeling/generate_silver_labels.py
```

**ë˜ëŠ” config ê¸°ë°˜:**
```bash
python3 scripts/generate_labels.py
```

### ì„¤ì •
- `config/config.yaml`ì˜ `silver_labeling` ì„¹ì…˜ ì°¸ê³ 
- **Hybrid approach**: keyword (30%) + semantic (70%) + top-down filtering
- **Top-down threshold**: 0.15 (Root: 0.105, Mid: 0.15, Leaf: 0.1)

### ìƒì„± íŒŒì¼
```
data/intermediate/
â”œâ”€â”€ train_silver_labels.pkl
â””â”€â”€ test_silver_labels.pkl
```

### ì˜ˆìƒ ê²°ê³¼
- Coverage: ~70-85%
- Average labels per sample: 2-4
- ì†Œìš” ì‹œê°„: 5-10ë¶„ (ë¡œì»¬), 3-5ë¶„ (AWS)

---

## ğŸ”¹ STEP 2: Baseline ëª¨ë¸ í•™ìŠµ

### Option A: ì§ì ‘ ëª…ë ¹ì–´ (ì¶”ì²œ)

```bash
python3 src/training/train_baseline.py \
  --model_type baseline \
  --model_name bert-base-uncased \
  --batch_size 16 \
  --num_epochs 3 \
  --learning_rate 2e-5 \
  --loss_type bce \
  --dropout 0.1 \
  --max_length 128 \
  --train_labels_path data/intermediate/train_silver_labels.pkl \
  --output_dir models/baseline
```

### Option B: Config ê¸°ë°˜

```bash
python3 scripts/train_with_config.py --config config/config.yaml
```

### ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | Default | ì˜µì…˜ |
|---------|------|---------|------|
| `--model_type` | ëª¨ë¸ íƒ€ì… (ë””ë ‰í† ë¦¬ ì´ë¦„) | `baseline` | baseline, gcn, gat, focal_loss ë“± |
| `--model_name` | BERT ëª¨ë¸ | `bert-base-uncased` | roberta-base, distilbert ë“± |
| `--batch_size` | ë°°ì¹˜ í¬ê¸° | `16` | 8, 16, 32 (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼) |
| `--num_epochs` | ì—í¬í¬ ìˆ˜ | `3` | 3-5 |
| `--learning_rate` | í•™ìŠµë¥  | `2e-5` | 1e-5 ~ 5e-5 |
| `--loss_type` | Loss í•¨ìˆ˜ | `bce` | bce, focal, asymmetric, kld |
| `--dropout` | Dropout ë¹„ìœ¨ | `0.1` | 0.1 ~ 0.3 |
| `--output_dir` | ì¶œë ¥ ë””ë ‰í† ë¦¬ | `models/{model_type}` | ìë™ ìƒì„± |

### ìƒì„± íŒŒì¼
```
models/baseline/
â”œâ”€â”€ checkpoint_epoch_1.pt
â”œâ”€â”€ checkpoint_epoch_2.pt
â”œâ”€â”€ checkpoint_epoch_3.pt
â”œâ”€â”€ best_model.pt              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”œâ”€â”€ final_model.pt             # ë§ˆì§€ë§‰ ì—í¬í¬ ëª¨ë¸
â””â”€â”€ training_history.json      # í•™ìŠµ íˆìŠ¤í† ë¦¬
```

### ì˜ˆìƒ ê²°ê³¼
- Training loss: 0.3-0.5
- ì†Œìš” ì‹œê°„: 2-3ì‹œê°„ (V100 GPU ê¸°ì¤€)

---

## ğŸ”¹ STEP 3: Self-Training (Optional)

### ëª…ë ¹ì–´

```bash
python3 src/training/train_baseline.py \
  --model_type baseline_self_training \
  --use_self_training \
  --self_training_confidence 0.7 \
  --self_training_iterations 3 \
  --num_epochs 2 \
  --batch_size 16 \
  --loss_type bce \
  --output_dir models/baseline_self_training
```

### Self-Training íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | Default |
|---------|------|---------|
| `--use_self_training` | Self-training í™œì„±í™” | False |
| `--self_training_confidence` | Pseudo-label confidence threshold | 0.7 |
| `--self_training_iterations` | ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ | 3 |

### ë™ì‘ ë°©ì‹
1. Labeled dataë¡œ ì´ˆê¸° í•™ìŠµ
2. Test dataì— ëŒ€í•´ soft pseudo-label ìƒì„± (confidence â‰¥ 0.7)
3. Labeled + Pseudo-labeled ë°ì´í„°ë¡œ ì¬í•™ìŠµ (KLD loss)
4. 2-3ë²ˆ ë°˜ë³µ ë˜ëŠ” ìˆ˜ë ´ ì‹œ ì¢…ë£Œ

### ìƒì„± íŒŒì¼
```
models/baseline_self_training/
â”œâ”€â”€ best_model.pt
â”œâ”€â”€ final_model.pt
â””â”€â”€ training_history.json
```

---

## ğŸ”¹ STEP 4: Inference (ì˜ˆì¸¡ ìƒì„±)

### ëª…ë ¹ì–´

```bash
python3 src/inference/predict.py \
  --model_path models/baseline/best_model.pt \
  --model_name baseline \
  --batch_size 32 \
  --threshold 0.3 \
  --device cuda
```

### íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | Default |
|---------|------|---------|
| `--model_path` | í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ | í•„ìˆ˜ |
| `--model_name` | ëª¨ë¸ ì´ë¦„ (ì¶œë ¥ íŒŒì¼ëª…ìš©) | baseline |
| `--batch_size` | ë°°ì¹˜ í¬ê¸° | 32 |
| `--threshold` | ì˜ˆì¸¡ threshold | 0.3 |
| `--device` | ë””ë°”ì´ìŠ¤ | cuda/mps/cpu |

### ìƒì„± íŒŒì¼
```
results/predictions/
â”œâ”€â”€ baseline_20251213_150430.pkl       # Pickle í˜•ì‹
â””â”€â”€ baseline_20251213_150430.csv       # CSV í˜•ì‹ (ì œì¶œìš©)
```

### CSV í˜•ì‹
```
# results/predictions/baseline_YYYYMMDD_HHMMSS.csv
# ê° ì¤„: space-separated class IDs
5 12 103 245
8 25 67 201 350
1 45 200
...
```

---

## ğŸ”¹ STEP 5: Submission íŒŒì¼ ìƒì„±

### ëª…ë ¹ì–´

```bash
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_20251213_150430.csv \
  --output results/submissions/20252R0136_baseline.csv \
  --student_id 20252R0136
```

### ë˜ëŠ” PKLì—ì„œ ì§ì ‘ ìƒì„±

```bash
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_20251213_150430.pkl \
  --output results/submissions/20252R0136_baseline.csv
```

### ìƒì„± íŒŒì¼
```
results/submissions/
â””â”€â”€ 20252R0136_baseline.csv    # Kaggle ì œì¶œ íŒŒì¼
```

---

## ìµœì¢… ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
data304_final/
â”œâ”€â”€ METHODOLOGY.md                      # ë°©ë²•ë¡  ë¬¸ì„œ
â”œâ”€â”€ PIPELINE.md                         # ì´ íŒŒì¼
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                     # ì „ì²´ ì„¤ì • íŒŒì¼
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ Amazon_products/            # ì›ë³¸ ë°ì´í„°
â”‚   â”‚       â”œâ”€â”€ classes.txt
â”‚   â”‚       â”œâ”€â”€ class_hierarchy.txt
â”‚   â”‚       â”œâ”€â”€ class_related_keywords.txt
â”‚   â”‚       â”œâ”€â”€ train/
â”‚   â”‚       â”‚   â””â”€â”€ train_corpus.txt
â”‚   â”‚       â””â”€â”€ test/
â”‚   â”‚           â””â”€â”€ test_corpus.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ train_silver_labels.pkl     # âœ… STEP 1 ìƒì„±
â”‚   â”‚   â””â”€â”€ test_silver_labels.pkl      # âœ… STEP 1 ìƒì„±
â”‚   â”‚
â”‚   â””â”€â”€ output/                          # (ë¯¸ì‚¬ìš©)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline/                        # âœ… STEP 2 ìƒì„±
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_1.pt
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_2.pt
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_3.pt
â”‚   â”‚   â”œâ”€â”€ best_model.pt               # ìµœê³  ì„±ëŠ¥
â”‚   â”‚   â”œâ”€â”€ final_model.pt
â”‚   â”‚   â””â”€â”€ training_history.json
â”‚   â”‚
â”‚   â”œâ”€â”€ baseline_self_training/          # âœ… STEP 3 ìƒì„± (optional)
â”‚   â”‚   â”œâ”€â”€ best_model.pt
â”‚   â”‚   â”œâ”€â”€ final_model.pt
â”‚   â”‚   â””â”€â”€ training_history.json
â”‚   â”‚
â”‚   â”œâ”€â”€ focal_loss/                      # ì¶”ê°€ ì‹¤í—˜ìš©
â”‚   â”œâ”€â”€ asymmetric_loss/                 # ì¶”ê°€ ì‹¤í—˜ìš©
â”‚   â”œâ”€â”€ gcn/                             # ì¶”ê°€ ì‹¤í—˜ìš©
â”‚   â””â”€â”€ gat/                             # ì¶”ê°€ ì‹¤í—˜ìš©
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ predictions/                     # âœ… STEP 4 ìƒì„±
â”‚   â”‚   â”œâ”€â”€ baseline_20251213_150430.pkl
â”‚   â”‚   â”œâ”€â”€ baseline_20251213_150430.csv
â”‚   â”‚   â”œâ”€â”€ baseline_self_training_20251213_163020.pkl
â”‚   â”‚   â””â”€â”€ baseline_self_training_20251213_163020.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ submissions/                     # âœ… STEP 5 ìƒì„±
â”‚   â”‚   â”œâ”€â”€ 20252R0136_baseline.csv      # ìµœì¢… ì œì¶œ íŒŒì¼
â”‚   â”‚   â””â”€â”€ 20252R0136_self_training.csv
â”‚   â”‚
â”‚   â””â”€â”€ images/                          # ì‹œê°í™” ê²°ê³¼
â”‚       â”œâ”€â”€ eda/
â”‚       â”œâ”€â”€ baseline/
â”‚       â”œâ”€â”€ ablation/
â”‚       â””â”€â”€ case_study/
â”‚
â”œâ”€â”€ logs/                                # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ training_20251213_150430.log
â”‚   â””â”€â”€ silver_labels_20251213_140230.log
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Visualization.ipynb
â”‚   â”œâ”€â”€ Ablation_Analysis.ipynb
â”‚   â””â”€â”€ CaseStudy.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_labels.py               # Silver label ìƒì„± (config ê¸°ë°˜)
â”‚   â”œâ”€â”€ train_with_config.py             # í•™ìŠµ (config ê¸°ë°˜)
â”‚   â”œâ”€â”€ generate_submission.py           # ì œì¶œ íŒŒì¼ ìƒì„±
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ test_silver_labels.py
â”‚       â”œâ”€â”€ test_training_pipeline.py
â”‚       â”œâ”€â”€ test_predict.py
â”‚       â””â”€â”€ test_submission.py
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ data_preprocessing.py
    â”œâ”€â”€ dataset.py
    â”‚
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ encoder.py                   # BERT TextEncoder
    â”‚   â”œâ”€â”€ classifier.py
    â”‚   â””â”€â”€ gnn_classifier.py
    â”‚
    â”œâ”€â”€ training/
    â”‚   â”œâ”€â”€ train_baseline.py            # â­ ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
    â”‚   â”œâ”€â”€ self_training.py             # â­ Self-training (soft labels + KLD)
    â”‚   â””â”€â”€ loss_functions.py            # BCE, Focal, Asymmetric, KLD
    â”‚
    â”œâ”€â”€ inference/
    â”‚   â”œâ”€â”€ predict.py                   # â­ ì˜ˆì¸¡ ìƒì„±
    â”‚   â””â”€â”€ dummy_baseline.py
    â”‚
    â”œâ”€â”€ silver_labeling/
    â”‚   â”œâ”€â”€ generate_silver_labels.py    # â­ Hybrid top-down labeling
    â”‚   â”œâ”€â”€ graph_utils.py
    â”‚   â””â”€â”€ llm_keyword_expansion.py
    â”‚
    â””â”€â”€ utils/
        â”œâ”€â”€ metrics.py
        â”œâ”€â”€ logger.py
        â”œâ”€â”€ seed.py
        â””â”€â”€ taxonomy_mapping.py
```

---

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì œ

### 1. ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# Step 1: Silver labels ìƒì„±
python3 src/silver_labeling/generate_silver_labels.py

# Step 2: Quick test (100 samples, 1 epoch)
python3 scripts/test/test_training_pipeline.py

# Step 3: Small training (ì „ì²´ ë°ì´í„°, 1 epoch for testing)
python3 src/training/train_baseline.py \
  --model_type baseline_test \
  --num_epochs 1 \
  --batch_size 8 \
  --output_dir models/baseline_test
```

### 2. AWS ì „ì²´ í•™ìŠµ

```bash
# Step 1: Silver labels (ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ)
python3 scripts/generate_labels.py

# Step 2: Baseline training
python3 src/training/train_baseline.py \
  --model_type baseline \
  --batch_size 16 \
  --num_epochs 3 \
  --output_dir models/baseline

# Step 3: Prediction
python3 src/inference/predict.py \
  --model_path models/baseline/best_model.pt \
  --model_name baseline

# Step 4: Generate submission
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_YYYYMMDD_HHMMSS.csv \
  --output results/submissions/20252R0136_baseline.csv
```

### 3. Self-Training ì‹¤í—˜

```bash
# Baseline with self-training
python3 src/training/train_baseline.py \
  --model_type baseline_self_training \
  --use_self_training \
  --self_training_confidence 0.7 \
  --self_training_iterations 3 \
  --num_epochs 2 \
  --batch_size 16

# Prediction
python3 src/inference/predict.py \
  --model_path models/baseline_self_training/best_model.pt \
  --model_name baseline_self_training

# Submission
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_self_training_YYYYMMDD_HHMMSS.csv \
  --output results/submissions/20252R0136_self_training.csv
```

---

## Ablation Studies (ì¶”ê°€ ì‹¤í—˜)

### Focal Loss

```bash
python3 src/training/train_baseline.py \
  --model_type focal_loss \
  --loss_type focal \
  --batch_size 16 \
  --num_epochs 3
```

### Asymmetric Loss

```bash
python3 src/training/train_baseline.py \
  --model_type asymmetric_loss \
  --loss_type asymmetric \
  --batch_size 16 \
  --num_epochs 3
```

### Different BERT Models

```bash
# RoBERTa
python3 src/training/train_baseline.py \
  --model_type roberta_baseline \
  --model_name roberta-base \
  --batch_size 16 \
  --num_epochs 3

# DistilBERT (faster)
python3 src/training/train_baseline.py \
  --model_type distilbert_baseline \
  --model_name distilbert-base-uncased \
  --batch_size 32 \
  --num_epochs 3
```

---

## ì˜ˆìƒ ì†Œìš” ì‹œê°„ (AWS p3.2xlarge - V100 GPU)

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ | ë¹„ê³  |
|-----|---------|-----|
| **Silver Label ìƒì„±** | 3-5ë¶„ | ìµœì´ˆ 1íšŒ |
| **Baseline Training (3 epochs)** | 2-3ì‹œê°„ | batch_size=16 |
| **Self-Training (3 iterations)** | 4-6ì‹œê°„ | iterationë‹¹ 1.5-2ì‹œê°„ |
| **Inference** | 10-15ë¶„ | batch_size=32 |
| **Total (baseline)** | ~3ì‹œê°„ | Silver + Train + Inference |
| **Total (self-training)** | ~6ì‹œê°„ | Silver + Self-Train + Inference |

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­

- [ ] ê°€ìƒí™˜ê²½ í™œì„±í™”: `source data304/bin/activate`
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜: `pip install -r requirements.txt`
- [ ] ë°ì´í„° ì¡´ì¬ í™•ì¸: `data/raw/Amazon_products/`
- [ ] Config ì„¤ì • í™•ì¸: `config/config.yaml`
- [ ] GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸: `nvidia-smi` (AWS)

### ì‹¤í–‰ í›„ í™•ì¸ì‚¬í•­

- [ ] Silver labels ìƒì„±: `data/intermediate/train_silver_labels.pkl`
- [ ] ëª¨ë¸ ì €ì¥: `models/{model_type}/best_model.pt`
- [ ] í•™ìŠµ íˆìŠ¤í† ë¦¬: `models/{model_type}/training_history.json`
- [ ] ì˜ˆì¸¡ íŒŒì¼: `results/predictions/*.csv`
- [ ] ì œì¶œ íŒŒì¼: `results/submissions/20252R0136_*.csv`

### ì œì¶œ ì „ í™•ì¸ì‚¬í•­

- [ ] CSV í¬ë§· í™•ì¸: ê° ì¤„ì´ space-separated integers
- [ ] ë¼ì¸ ìˆ˜ í™•ì¸: 19,658 lines (test set size)
- [ ] íŒŒì¼ëª… í˜•ì‹: `20252R0136_*.csv`
- [ ] íŒŒì¼ í¬ê¸°: ~1-5MB

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Out of Memory

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--batch_size 8

# Gradient accumulation ì‚¬ìš©
--gradient_accumulation_steps 2
```

### Slow Training

```bash
# DistilBERT ì‚¬ìš© (50% faster)
--model_name distilbert-base-uncased

# Mixed precision training (AWS GPU only)
# config.yamlì—ì„œ mixed_precision: true
```

### Poor Performance

```bash
# ë” ë§ì€ epoch
--num_epochs 5

# Self-training ì‚¬ìš©
--use_self_training

# Different loss function
--loss_type focal  # or asymmetric
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. **EDA ë…¸íŠ¸ë¶ ì‹¤í–‰**: `notebooks/EDA.ipynb`
2. **Ablation ë¶„ì„**: `notebooks/Ablation_Analysis.ipynb`
3. **Case Study**: `notebooks/CaseStudy.ipynb`
4. **ê²°ê³¼ ì‹œê°í™”**: `notebooks/Visualization.ipynb`
5. **ë ˆí¬íŠ¸ ì‘ì„±**: ê²°ê³¼ ì •ë¦¬ ë° ë¶„ì„

---

## ì°¸ê³  ë¬¸ì„œ

- **METHODOLOGY.md**: ì „ì²´ ë°©ë²•ë¡  ìƒì„¸ ì„¤ëª…
- **README.md**: í”„ë¡œì íŠ¸ ê°œìš”
- **config/config.yaml**: ì„¤ì • íŒŒì¼ ì£¼ì„
