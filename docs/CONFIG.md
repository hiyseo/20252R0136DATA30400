# Configuration Guide

## Overview

This project uses a centralized YAML configuration system (`config/config.yaml`) for managing all hyperparameters, paths, and experimental settings. The configuration supports **baseline training with 2-stage self-training** (BCE initialization + KLD self-training).

---

## Configuration Structure

### 1. Data Configuration

```yaml
data:
  data_dir: "data/raw/Amazon_products"
  train_labels_path: "data/intermediate/train_silver_labels.pkl"
  test_labels_path: "data/intermediate/test_silver_labels.pkl"
  max_length: 128          # Maximum token length for BERT
  num_workers: 4           # DataLoader workers
```

**Key Parameters:**
- `data_dir`: Raw Amazon products data directory
- `train_labels_path`: Silver labels for training (generated by `generate_labels.py`)
- `test_labels_path`: Silver labels for test set (used for pseudo-labeling)
- `max_length`: Input sequence length (128 for efficiency, 512 for full context)

---

### 2. Silver Labeling Configuration

```yaml
silver_labeling:
  embedding_model: "sentence-transformers/all-mpnet-base-v2"
  use_keyword_matching: true
  use_semantic_similarity: true
  use_topdown_filtering: true     # Hybrid top-down approach
  keyword_weight: 0.3              # Keyword score weight
  similarity_weight: 0.7           # Semantic similarity weight
  min_confidence: 0.1
  similarity_threshold: 0.5
  topdown_threshold: 0.15          # Top-down filtering threshold
  batch_size: 32
```

**Hybrid Top-Down Approach:**
1. **Score Computation**: `combined_score = 0.3 × keyword + 0.7 × semantic`
2. **Top-Down Filtering**: Level-by-level filtering with thresholds
   - Root level: 0.105
   - Mid level: 0.15
   - Leaf level: 0.1

**When to Modify:**
- Increase `keyword_weight` if keyword matching is more important
- Increase `topdown_threshold` to be more conservative in label assignment
- Disable `use_topdown_filtering` to use simple thresholding

---

### 3. Model Configuration

```yaml
model:
  model_name: "bert-base-uncased"
  model_type: "baseline"           # Used in output paths via {model_type}
  num_classes: 531
  dropout: 0.1
  freeze_encoder: false
  
  # GNN models (optional)
  gnn_hidden_dim: 512
  gnn_num_layers: 2
  gnn_num_heads: 4
```

**Key Parameters:**
- `model_type`: Identifier for experiments (e.g., "baseline", "focal_loss", "gcn")
- `freeze_encoder`: Set to `true` to only train classification head
- GNN parameters: Used only for GNN-based models (not baseline)

**Model Type Examples:**
```yaml
# Standard baseline
model_type: "baseline"

# Focal loss experiment
model_type: "focal_loss"

# GCN experiment
model_type: "gcn"
```

---

### 4. Self-Training Configuration (⭐ Baseline Setting)

```yaml
self_training:
  enabled: true                    # Baseline uses self-training
  confidence_threshold: 0.7        # Minimum confidence for pseudo-labels
  max_iterations: 3                # Self-training iterations
  min_improvement: 0.001           # Early stopping threshold
  epochs_per_iteration: 1          # Epochs per self-training iteration
```

**2-Stage Training Process:**

**Stage 1: BCE Initialization**
- Train model on labeled data with BCE loss
- Duration: `num_epochs` (default: 2)
- Purpose: Initialize model parameters for pseudo-labeling

**Stage 2: KLD Self-Training**
- Generate soft pseudo-labels (confidence ≥ 0.7)
- Combine labeled + pseudo-labeled data
- Train with KL Divergence loss
- Repeat for `max_iterations` (default: 3)

**To Disable Self-Training:**
```yaml
self_training:
  enabled: false  # Standard BCE training only
```

---

### 5. Training Configuration

```yaml
training:
  batch_size: 16
  num_epochs: 2                    # Stage 1 initialization epochs
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1                # 10% warmup steps
  gradient_clip_norm: 1.0
  
  loss_type: "bce"                 # Stage 1: BCE, Stage 2: KLD (automatic)
  
  # Focal loss parameters (if loss_type: "focal")
  focal_alpha: 0.25
  focal_gamma: 2.0
  
  # Hierarchical loss parameters (if loss_type: "hierarchical")
  lambda_hier: 0.1
```

**Key Parameters:**
- `num_epochs`: Used for Stage 1 BCE initialization (reduced to 2 for efficiency)
- `loss_type`: BCE for Stage 1, KLD automatically applied in Stage 2
- `warmup_ratio`: Linear warmup for stable training

**Loss Functions:**
- `bce`: Binary Cross-Entropy (default, Stage 1)
- `focal`: Focal Loss (handles class imbalance)
- `asymmetric`: Asymmetric Loss (optimizes for multi-label)
- `kld`: KL Divergence (Stage 2, automatic in self-training)

---

### 6. Output Configuration

```yaml
output:
  output_dir: "models/{model_type}"              # Placeholder replaced
  predictions_dir: "results/predictions"
  images_dir: "results/images/{model_type}"
  log_dir: "logs"
  save_predictions: true
```

**Placeholder System:**
The `{model_type}` placeholder is **automatically replaced** with `model.model_type` value:

```yaml
model_type: "baseline" → output_dir: "models/baseline"
model_type: "focal_loss" → output_dir: "models/focal_loss"
model_type: "gcn" → output_dir: "models/gcn"
```

This allows easy organization of experiments without manual path changes.

---

## Usage Examples

### Example 1: Standard Baseline (Current Setting)

```yaml
model:
  model_type: "baseline"

self_training:
  enabled: true

training:
  num_epochs: 2
  loss_type: "bce"
```

**Result:**
- Stage 1: 2 epochs BCE training
- Stage 2: 3 iterations self-training with KLD
- Output: `models/baseline/best_model.pt`

### Example 2: Focal Loss Experiment

```yaml
model:
  model_type: "focal_loss"

self_training:
  enabled: false

training:
  num_epochs: 5
  loss_type: "focal"
  focal_alpha: 0.25
  focal_gamma: 2.0
```

**Result:**
- 5 epochs standard training with Focal Loss
- No self-training
- Output: `models/focal_loss/best_model.pt`

### Example 3: Aggressive Self-Training

```yaml
model:
  model_type: "aggressive_st"

self_training:
  enabled: true
  confidence_threshold: 0.5        # Lower threshold
  max_iterations: 5                # More iterations

training:
  num_epochs: 3
```

**Result:**
- Stage 1: 3 epochs BCE
- Stage 2: 5 iterations with confidence ≥ 0.5
- Output: `models/aggressive_st/best_model.pt`

---

## Command-Line Overrides

The config file can be overridden via command-line arguments:

```bash
# Override batch size
python3 scripts/train_with_config.py --batch_size 32

# Override epochs
python3 scripts/train_with_config.py --num_epochs 5

# Force self-training
python3 scripts/train_with_config.py --use_self_training

# Override learning rate
python3 scripts/train_with_config.py --learning_rate 3e-5

# Multiple overrides
python3 scripts/train_with_config.py \
  --batch_size 32 \
  --num_epochs 4 \
  --use_self_training
```

---

## Configuration Validation

Before training, validate your configuration:

```bash
# Check config loading
python3 -c "
import yaml
with open('config/config.yaml') as f:
    config = yaml.safe_load(f)
print('Model type:', config['model']['model_type'])
print('Self-training:', config['self_training']['enabled'])
print('Epochs:', config['training']['num_epochs'])
"
```

Expected output:
```
Model type: baseline
Self-training: True
Epochs: 2
```

---

## Best Practices

### 1. Experiment Organization

Use descriptive `model_type` names:
```yaml
model_type: "baseline_v1"          # First baseline attempt
model_type: "baseline_v2"          # Improved baseline
model_type: "focal_alpha025"       # Focal loss with α=0.25
model_type: "gcn_2layers"          # GCN with 2 layers
```

### 2. Hyperparameter Tuning

**For self-training:**
- Start with `confidence_threshold: 0.7` (conservative)
- Try `0.5` or `0.8` based on initial results
- Increase `max_iterations` if loss keeps improving

**For BCE training:**
- Standard: `num_epochs: 3-5`, `learning_rate: 2e-5`
- Fast iteration: `num_epochs: 2`, `learning_rate: 3e-5`
- Fine-tuning: `num_epochs: 10`, `learning_rate: 1e-5`

### 3. Resource Management

**GPU Training:**
```yaml
training:
  batch_size: 32               # Larger batch on GPU
  mixed_precision: true        # Enable FP16

misc:
  device: "cuda"
```

**CPU/MPS Training:**
```yaml
training:
  batch_size: 8                # Smaller batch
  mixed_precision: false       # Disable FP16

misc:
  device: "mps"                # For Apple Silicon
```

---

## Troubleshooting

### Issue: Out of Memory

**Solution 1: Reduce batch size**
```yaml
training:
  batch_size: 8  # Or 4
```

**Solution 2: Reduce max length**
```yaml
data:
  max_length: 64  # Instead of 128
```

### Issue: Training too slow

**Solution 1: Reduce epochs**
```yaml
training:
  num_epochs: 2  # Fast iteration

self_training:
  max_iterations: 2  # Fewer self-training iterations
```

**Solution 2: Increase batch size**
```yaml
training:
  batch_size: 32  # If GPU memory allows
```

### Issue: Poor performance

**Solution 1: Increase training**
```yaml
training:
  num_epochs: 5  # More initial training

self_training:
  max_iterations: 5  # More refinement
```

**Solution 2: Adjust confidence**
```yaml
self_training:
  confidence_threshold: 0.5  # Accept more pseudo-labels
```

---

## Related Files

- **Main config**: `config/config.yaml`
- **Training script**: `scripts/train_with_config.py`
- **Direct training**: `src/training/train_baseline.py`
- **Label generation**: `scripts/generate_labels.py`
- **Pipeline guide**: `docs/PIPELINE.md`
