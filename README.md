# Amazon ìƒí’ˆ ê³„ì¸µ ë¶„ë¥˜ (Hierarchical Product Classification)

**í•™ë²ˆ:** 20252R0136 | **ê³¼ëª©:** DATA304

---

## ğŸ“‹ ê°œìš”

Amazon ìƒí’ˆì„ 531ê°œ í´ë˜ìŠ¤ë¡œ ìë™ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ë°©ë²•**: Silver Label ìƒì„± â†’ BCE í•™ìŠµ â†’ Self-Training (KLD)  
**íŠ¹ì§•**: ë ˆì´ë¸” ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
data304_final/
â”œâ”€â”€ config/                      # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ config.yaml             # í†µí•© ì‹¤í—˜ ì„¤ì •
â”‚
â”œâ”€â”€ data/                        # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ raw/                    # ì›ë³¸ ë°ì´í„° (Amazon Products)
â”‚   â”‚   â””â”€â”€ Amazon_products/    # Amazon ìƒí’ˆ ë°ì´í„°ì…‹
â”‚   â”‚       â”œâ”€â”€ train/          # í•™ìŠµ ë°ì´í„° (corpus)
â”‚   â”‚       â”œâ”€â”€ test/           # í…ŒìŠ¤íŠ¸ ë°ì´í„° (corpus)
â”‚   â”‚       â”œâ”€â”€ classes.txt     # 531ê°œ í´ë˜ìŠ¤ ëª©ë¡
â”‚   â”‚       â”œâ”€â”€ class_hierarchy.txt  # ê³„ì¸µ êµ¬ì¡° (parent-child)
â”‚   â”‚       â””â”€â”€ class_related_keywords.txt  # í´ë˜ìŠ¤ë³„ í‚¤ì›Œë“œ
â”‚   â”œâ”€â”€ intermediate/           # ì¤‘ê°„ ì²˜ë¦¬ ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ train_silver_labels.pkl  # í•™ìŠµ ë°ì´í„° Silver label
â”‚   â”‚   â””â”€â”€ test_silver_labels.pkl   # í…ŒìŠ¤íŠ¸ ë°ì´í„° Silver label
â”‚   â””â”€â”€ output/                 # ìµœì¢… ì¶œë ¥ (ì˜ˆì¸¡ ê²°ê³¼)
â”‚
â”œâ”€â”€ src/                         # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ data_preprocessing.py   # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ models/                 # ëª¨ë¸ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ classifier.py       # BERT ê¸°ë°˜ ë¶„ë¥˜ê¸°
â”‚   â”‚   â”œâ”€â”€ encoder.py          # BERT ì¸ì½”ë”
â”‚   â”‚   â””â”€â”€ gnn_classifier.py   # GNN ëª¨ë¸ (Graph Neural Network)
â”‚   â”œâ”€â”€ silver_labeling/        # Silver label ìƒì„±
â”‚   â”‚   â”œâ”€â”€ generate_silver_labels.py  # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”‚   â”‚   â”œâ”€â”€ graph_utils.py      # ê³„ì¸µ ê·¸ë˜í”„ ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ llm_keyword_expansion.py  # LLM í‚¤ì›Œë“œ í™•ì¥ (ì„ íƒ)
â”‚   â”œâ”€â”€ training/               # í•™ìŠµ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ train_baseline.py   # 2ë‹¨ê³„ í•™ìŠµ (BCE â†’ Self-Training)
â”‚   â”‚   â”œâ”€â”€ self_training.py    # Self-training êµ¬í˜„
â”‚   â”‚   â””â”€â”€ loss_functions.py   # ì†ì‹¤ í•¨ìˆ˜ (BCE, KLD, Focal)
â”‚   â”œâ”€â”€ inference/              # ì˜ˆì¸¡ ìƒì„±
â”‚   â”‚   â”œâ”€â”€ predict.py          # ëª¨ë¸ ì˜ˆì¸¡
â”‚   â”‚   â””â”€â”€ dummy_baseline.py   # ë”ë¯¸ ë² ì´ìŠ¤ë¼ì¸
â”‚   â””â”€â”€ utils/                  # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ metrics.py          # í‰ê°€ ì§€í‘œ (F1, Precision, Recall)
â”‚       â”œâ”€â”€ taxonomy_mapping.py # ê³„ì¸µ ë§¤í•‘
â”‚       â”œâ”€â”€ logger.py           # ë¡œê¹…
â”‚       â””â”€â”€ seed.py             # ëœë¤ ì‹œë“œ ê³ ì •
â”‚
â”œâ”€â”€ scripts/                     # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ generate_labels.py      # Silver label ìƒì„± ì‹¤í–‰
â”‚   â”œâ”€â”€ train_with_config.py    # Config ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰
â”‚   â””â”€â”€ generate_submission.py  # ì œì¶œ íŒŒì¼ ìƒì„±
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter Notebook ë¶„ì„
â”‚   â”œâ”€â”€ EDA.ipynb               # ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
â”‚   â”œâ”€â”€ Ablation_Analysis.ipynb # ì‹¤í—˜ ê²°ê³¼ ë¹„êµ
â”‚   â””â”€â”€ CaseStudy.ipynb         # ì˜ˆì¸¡ ì˜¤ë¥˜ ë¶„ì„
â”‚
â”œâ”€â”€ docs/                        # ë¬¸ì„œ
â”‚   â”œâ”€â”€ CONFIG.md               # Config ìƒì„¸ ì„¤ëª…
â”‚   â”œâ”€â”€ PIPELINE.md             # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ì´ë“œ
â”‚   â””â”€â”€ METHODOLOGY.md          # ë°©ë²•ë¡  ë° ìˆ˜ì‹
â”‚
â”œâ”€â”€ models/                      # í•™ìŠµëœ ëª¨ë¸ (ìë™ ìƒì„±)
â”‚   â””â”€â”€ {model_type}/           # ì‹¤í—˜ë³„ í´ë”
â”‚       â”œâ”€â”€ best_model.pt       # ìµœì¢… ëª¨ë¸
â”‚       â”œâ”€â”€ training_history.json  # í•™ìŠµ ê¸°ë¡
â”‚       â””â”€â”€ checkpoint_*.pt     # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚
â”œâ”€â”€ results/                     # ê²°ê³¼ íŒŒì¼ (ìë™ ìƒì„±)
â”‚   â”œâ”€â”€ predictions/            # ì˜ˆì¸¡ ê²°ê³¼ (pkl, csv)
â”‚   â”œâ”€â”€ submissions/            # ì œì¶œ íŒŒì¼
â”‚   â””â”€â”€ images/                 # ì‹œê°í™” ì´ë¯¸ì§€
â”‚
â”œâ”€â”€ logs/                        # ë¡œê·¸ íŒŒì¼ (ìë™ ìƒì„±)
â”‚
â”œâ”€â”€ run.sh                       # ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt             # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â””â”€â”€ README.md                    # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
```

**í•µì‹¬ ë””ë ‰í† ë¦¬:**
- `src/`: ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ (ëª¨ë¸, í•™ìŠµ, ì¶”ë¡ )
- `config/`: ì‹¤í—˜ ì„¤ì • (ë‹¨ì¼ YAML íŒŒì¼)
- `data/`: ì›ë³¸ â†’ ì¤‘ê°„ â†’ ìµœì¢… ë°ì´í„° íë¦„
- `scripts/`: ì‹¤í–‰ ì§„ì…ì  (config ê¸°ë°˜)
- `models/`: í•™ìŠµëœ ëª¨ë¸ ì €ì¥
- `results/`: ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv data304
source data304/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

**í•„ìš”**: Python 3.10+, 16GB RAM, 10GB ì €ì¥ê³µê°„

### 2. ì „ì²´ ì‹¤í–‰ (í•œ ë²ˆì—)

```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ (ìµœì´ˆ 1íšŒ)
chmod +x run.sh

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
./run.sh
```

**ì†Œìš” ì‹œê°„**: CPU 12-16ì‹œê°„, GPU 3-4ì‹œê°„

### 3. ë‹¨ê³„ë³„ ì‹¤í–‰ (ì„ íƒ)

```bash
# 1. ë°ì´í„° ì „ì²˜ë¦¬
python3 src/data_preprocessing.py

# 2. Silver Label ìƒì„± (30-45ë¶„)
python3 scripts/generate_labels.py

# 3. ëª¨ë¸ í•™ìŠµ (Stage 1: BCE â†’ Stage 2: Self-Training)
python3 scripts/train_with_config.py

# 4. ì˜ˆì¸¡ ìƒì„±
python3 src/inference/predict.py \
  --model_path models/baseline/best_model.pt \
  --model_name baseline

# 5. ì œì¶œ íŒŒì¼ ìƒì„±
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_*.csv \
  --output results/submissions/20252R0136_baseline.csv
```

**ìµœì¢… ì¶œë ¥**: `results/submissions/20252R0136_baseline.csv` (ì œì¶œìš©)

---

## ğŸ–¥ï¸ ì‹¤í–‰ í™˜ê²½

### ë¡œì»¬ (CPU/GPU)

```yaml
# config/config.yaml
misc:
  device: "auto"  # ë˜ëŠ” "cpu", "cuda", "mps"
  
training:
  batch_size: 16  # CPUëŠ” 8, GPUëŠ” 32
```

**ì˜ˆìƒ ì‹œê°„**: CPU 12-16ì‹œê°„, GPU 3-6ì‹œê°„

---

## âš™ï¸ Config ì„¤ì •

`config/config.yaml` íŒŒì¼ ìˆ˜ì •ìœ¼ë¡œ ì‹¤í—˜ ì„¤ì •:

### ì£¼ìš” ì˜µì…˜

```yaml
# ëª¨ë¸ ì„¤ì •
model:
  model_name: "bert-base-uncased"  # ì‚¬ì „í•™ìŠµ ëª¨ë¸
    # ì˜µì…˜: "bert-base-uncased", "roberta-base", "distilbert-base-uncased"
  model_type: "baseline"  # ì‹¤í—˜ ì´ë¦„ (ì¶œë ¥ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©)
    # ì˜ˆì‹œ: "baseline", "focal_loss", "gcn", "gat", "no_self_training"
  dropout: 0.1  # Dropout ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)

# í•™ìŠµ ì„¤ì •
training:
  batch_size: 16  # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    # CPU: 4-8, GPU (8GB): 16, GPU (16GB+): 32
  num_epochs: 2  # Stage 1 ì´ˆê¸° í•™ìŠµ ì—í¬í¬
  learning_rate: 2.0e-5  # í•™ìŠµë¥ 
  loss_type: "bce"  # ì†ì‹¤ í•¨ìˆ˜
    # ì˜µì…˜: "bce" (Binary Cross Entropy), "focal" (Focal Loss)
  
  # Focal Loss ì„¤ì • (loss_type: "focal"ì¼ ë•Œ)
  focal_alpha: 0.25  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •
  focal_gamma: 2.0   # ì‰¬ìš´ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê°ì†Œ

# Self-Training ì„¤ì •
self_training:
  enabled: true  # Self-training í™œì„±í™”
    # true: BCE (Stage 1) â†’ KLD (Stage 2)
    # false: BCEë§Œ ì‚¬ìš©
  confidence_threshold: 0.7  # Pseudo-label ì‹ ë¢°ë„ ì„ê³„ê°’
    # ë†’ì„ìˆ˜ë¡ ì—„ê²© (0.6-0.9 ê¶Œì¥)
  max_iterations: 3  # Self-training ë°˜ë³µ íšŸìˆ˜

# ë°ì´í„° ì„¤ì •
data:
  max_length: 128  # í…ìŠ¤íŠ¸ ìµœëŒ€ í† í° ê¸¸ì´
    # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: 64, ê¸´ í…ìŠ¤íŠ¸: 256
  num_workers: 4  # ë°ì´í„° ë¡œë”© ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜

# í™˜ê²½ ì„¤ì •
misc:
  device: "auto"  # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
    # ì˜µì…˜: "auto", "cpu", "cuda" (NVIDIA GPU), "mps" (Apple Silicon)
  seed: 42  # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
  mixed_precision: true  # í˜¼í•© ì •ë°€ë„ í•™ìŠµ (GPU ì†ë„ í–¥ìƒ)
```

### ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì •

**ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (5-10ë¶„)**
```yaml
training:
  num_epochs: 1
  batch_size: 8
self_training:
  enabled: false
```

**ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**
```yaml
data:
  max_length: 64
  batch_size: 4
misc:
  mixed_precision: true
```

**ê³ ì„±ëŠ¥ GPU (ê¸´ í•™ìŠµ)**
```yaml
training:
  num_epochs: 5
  batch_size: 32
data:
  max_length: 256
self_training:
  max_iterations: 5
```

**Focal Loss ì‹¤í—˜**
```yaml
model:
  model_type: "focal_loss"
training:
  loss_type: "focal"
  focal_alpha: 0.25
  focal_gamma: 2.0
self_training:
  enabled: false
```

**ìƒì„¸ ì„¤ëª…**: `docs/CONFIG.md` ì°¸ì¡°

---

## ğŸ”‘ LLM API ì„¤ì • (ì„ íƒì‚¬í•­)

í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ OpenAI API ì„¤ì • (ì„ íƒ):

```bash
# 1. API í‚¤ ë°œê¸‰: https://platform.openai.com/
# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "OPENAI_API_KEY=sk-proj-..." > .env

# 3. Config í™œì„±í™”
# config/config.yaml
silver_labeling:
  llm_expansion:
    enabled: true
    model: "gpt-4o-mini"
    max_calls: 1000  # ë¹„ìš© ì œí•œ

# 4. ì‹¤í–‰
python3 src/silver_labeling/llm_keyword_expansion.py
```

**ë¹„ìš©**: 100-200 í´ë˜ìŠ¤ í™•ì¥ ì‹œ $2-5  
**ì„ íƒì‚¬í•­**: ì—†ì–´ë„ ì •ìƒ ì‘ë™ (ê¸°ë³¸ í‚¤ì›Œë“œë¡œë„ ì¶©ë¶„)

---

## ğŸ“Š ë°©ë²•ë¡ 

### 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

1. **Silver Label ìƒì„±**: í‚¤ì›Œë“œ ë§¤ì¹­(30%) + ì„ë² ë”© ìœ ì‚¬ë„(70%)
2. **Stage 1 (BCE)**: Hard labelë¡œ ì´ˆê¸° í•™ìŠµ (2 epochs)
3. **Stage 2 (KLD)**: Soft pseudo-labelë¡œ self-training (3 iterations)

---

## ğŸ”¬ ì‹¤í—˜ ë° ë¶„ì„

Ablation studyë¥¼ ìœ„í•œ Jupyter Notebook ì œê³µ:

```bash
jupyter notebook notebooks/Ablation_Analysis.ipynb  # ì‹¤í—˜ ë¹„êµ
jupyter notebook notebooks/CaseStudy.ipynb          # ì˜ˆì¸¡ ë¶„ì„
jupyter notebook notebooks/EDA.ipynb                # ë°ì´í„° íƒìƒ‰
```

**ì‹¤í—˜ ì˜ˆì‹œ**:
- Self-training íš¨ê³¼: `self_training.enabled: false`
- ì„ê³„ê°’ ë³€ê²½: `confidence_threshold: 0.8`
- Loss í•¨ìˆ˜ ë¹„êµ: `loss_type: "focal"`

---