# ğŸ›ï¸ Amazon Hierarchical Product Classification

**í•™ë²ˆ:** 2020320135 | **ê³¼ëª©:** DATA304

---

## ğŸ“‹ ê°œìš”

Amazon ìƒí’ˆì„ 531ê°œ í´ë˜ìŠ¤ë¡œ ìë™ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ë°©ë²•**: Silver Label ìƒì„± â†’ BCE í•™ìŠµ â†’ Self-Training (KLD)  
**íŠ¹ì§•**: ë ˆì´ë¸” ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

**í•µì‹¬ ë””ë ‰í† ë¦¬:**
- `src/`: ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œ (ëª¨ë¸, í•™ìŠµ, ì¶”ë¡ )
- `config/`: ì‹¤í—˜ ì„¤ì • (ë‹¨ì¼ YAML íŒŒì¼)
- `data/`: ì›ë³¸ â†’ ì¤‘ê°„ â†’ ìµœì¢… ë°ì´í„° íë¦„
- `scripts/`: ì‹¤í–‰ ì§„ì…ì  (config ê¸°ë°˜)
- `models/`: í•™ìŠµëœ ëª¨ë¸ ì €ì¥
- `results/`: ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv data304
source data304/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

**í•„ìš”**: Python 3.10+, 16GB RAM, 10GB ì €ì¥ê³µê°„

### 2. ì „ì²´ ì‹¤í–‰ (í•œ ë²ˆì—)

```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ (ìµœì´ˆ 1íšŒ)
chmod +x run.sh

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
./run.sh
```

**ì†Œìš” ì‹œê°„**: CPU 12-16ì‹œê°„, GPU 3-4ì‹œê°„

### 3. ë‹¨ê³„ë³„ ì‹¤í–‰ (ì„ íƒ)

```bash
# 1. ë°ì´í„° ì „ì²˜ë¦¬
python3 src/data_preprocessing.py

# 2. Silver Label ìƒì„± (30-45ë¶„)
python3 scripts/generate_labels.py

# 3. ëª¨ë¸ í•™ìŠµ (Stage 1: BCE â†’ Stage 2: Self-Training)
python3 scripts/train_with_config.py

# 4. ì˜ˆì¸¡ ìƒì„±
python3 src/inference/predict.py \
  --model_path models/baseline/best_model.pt \
  --model_name baseline

# 5. ì œì¶œ íŒŒì¼ ìƒì„±
python3 scripts/generate_submission.py \
  --predictions results/predictions/baseline_*.csv \
  --output results/submissions/20252R0136_baseline.csv
```

**ìµœì¢… ì¶œë ¥**: `results/submissions/20252R0136_baseline.csv` (ì œì¶œìš©)

---

## ğŸ–¥ï¸ ì‹¤í–‰ í™˜ê²½

### ë¡œì»¬ (CPU/GPU)

```yaml
# config/config.yaml
misc:
  device: "auto"  # ë˜ëŠ” "cpu", "cuda", "mps"
  
training:
  batch_size: 16  # CPUëŠ” 8, GPUëŠ” 32
```

**ì˜ˆìƒ ì‹œê°„**: CPU 12-16ì‹œê°„, GPU 3-6ì‹œê°„

---

### ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì • (Model Type)

#### 1. Baseline (2-Stage Training)
```yaml
model:
  model_type: "baseline"
training:
  loss_type: "bce"
  num_epochs: 2
self_training:
  enabled: true  # BCE â†’ Self-Training (KLD)
  confidence_threshold: 0.7
  max_iterations: 3
```

#### 2. Focal Loss (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)
```yaml
model:
  model_type: "focal_loss"
training:
  loss_type: "focal"
  focal_alpha: 0.25
  focal_gamma: 2.0
  num_epochs: 5
self_training:
  enabled: false
```

#### 3. Self-Training ì—†ì´ (BCEë§Œ)
```yaml
model:
  model_type: "no_self_training"
training:
  loss_type: "bce"
  num_epochs: 5
self_training:
  enabled: false
```

#### 4. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
```yaml
model:
  model_type: "quick_test"
training:
  num_epochs: 1
  batch_size: 8
self_training:
  enabled: false
```

**ìƒì„¸ ì„¤ëª…**: `docs/CONFIG.md` ì°¸ì¡°

---

## ğŸ”‘ LLM API ì„¤ì • (ì„ íƒì‚¬í•­)

í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ OpenAI API ì„¤ì • (ì„ íƒ):

```bash
# 1. API í‚¤ ë°œê¸‰: https://platform.openai.com/
# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "OPENAI_API_KEY=sk-proj-..." > .env

# 3. Config í™œì„±í™”
# config/config.yaml
silver_labeling:
  llm_expansion:
    enabled: true
    model: "gpt-4o-mini"
    max_calls: 1000  # ë¹„ìš© ì œí•œ

# 4. ì‹¤í–‰
python3 src/silver_labeling/llm_keyword_expansion.py
```

**ìƒì„¸ ì„¤ëª…**: `docs/LLM_KEYWORD_EXPANSION.md` ì°¸ì¡°

---

## ğŸ“Š ë°©ë²•ë¡ 

### 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

1. **Silver Label ìƒì„±**: í‚¤ì›Œë“œ ë§¤ì¹­(30%) + ì„ë² ë”© ìœ ì‚¬ë„(70%)
2. **Stage 1 (BCE)**: Hard labelë¡œ ì´ˆê¸° í•™ìŠµ (2 epochs)
3. **Stage 2 (KLD)**: Soft pseudo-labelë¡œ self-training (3 iterations)

---

## ğŸ”¬ ì‹¤í—˜ ë° ë¶„ì„

Ablation studyë¥¼ ìœ„í•œ Jupyter Notebook ì œê³µ:

```bash
jupyter notebook notebooks/Ablation_Analysis.ipynb  # ì‹¤í—˜ ë¹„êµ
jupyter notebook notebooks/CaseStudy.ipynb          # ì˜ˆì¸¡ ë¶„ì„
jupyter notebook notebooks/EDA.ipynb                # ë°ì´í„° íƒìƒ‰
```

**ì‹¤í—˜ ì˜ˆì‹œ**:
- Self-training íš¨ê³¼: `self_training.enabled: false`
- ì„ê³„ê°’ ë³€ê²½: `confidence_threshold: 0.8`
- Loss í•¨ìˆ˜ ë¹„êµ: `loss_type: "focal"`

---