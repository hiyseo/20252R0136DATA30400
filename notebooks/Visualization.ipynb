{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION: Change this for different models\n",
    "# ============================================================\n",
    "MODEL_NAME = 'baseline'  # 'baseline', 'gcn', 'gat', 'focal_loss', etc.\n",
    "\n",
    "# Directories\n",
    "models_dir = Path('../models')\n",
    "predictions_dir = Path('../results/predictions')\n",
    "fig_dir = Path(f'../results/images/{MODEL_NAME}')\n",
    "comparison_dir = Path('../results/images/ablation')\n",
    "\n",
    "# Create directories\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Visualization for: {MODEL_NAME.upper()}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✓ Images will be saved to: {fig_dir}\")\n",
    "print(f\"✓ Comparison plots to: {comparison_dir}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f05b9",
   "metadata": {},
   "source": [
    "## 1. Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "model_path = models_dir / MODEL_NAME / 'training_history.json'\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"⚠️  Training history not found: {model_path}\")\n",
    "    print(f\"   Available models: {[d.name for d in models_dir.iterdir() if d.is_dir()]}\")\n",
    "else:\n",
    "    with open(model_path) as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    # Plot train and validation loss\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Training Loss\n",
    "    axes[0].plot(history['epochs'], history['train_loss'], \n",
    "                 marker='o', linewidth=2.5, label='Training Loss', color='#2E86AB')\n",
    "    if 'val_loss' in history:\n",
    "        axes[0].plot(history['epochs'], history['val_loss'], \n",
    "                     marker='s', linewidth=2.5, label='Validation Loss', color='#A23B72')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=13)\n",
    "    axes[0].set_ylabel('Loss', fontsize=13)\n",
    "    axes[0].set_title(f'{MODEL_NAME.upper()} - Training Loss', fontsize=15, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Metrics (if available)\n",
    "    if 'train_f1' in history:\n",
    "        axes[1].plot(history['epochs'], history['train_f1'], \n",
    "                     marker='o', linewidth=2.5, label='Train F1', color='#2E86AB')\n",
    "        if 'val_f1' in history:\n",
    "            axes[1].plot(history['epochs'], history['val_f1'], \n",
    "                         marker='s', linewidth=2.5, label='Val F1', color='#A23B72')\n",
    "        axes[1].set_xlabel('Epoch', fontsize=13)\n",
    "        axes[1].set_ylabel('F1 Score', fontsize=13)\n",
    "        axes[1].set_title(f'{MODEL_NAME.upper()} - F1 Score', fontsize=15, fontweight='bold')\n",
    "        axes[1].legend(fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'F1 metrics not available', \n",
    "                     ha='center', va='center', fontsize=12, color='gray')\n",
    "        axes[1].set_title('F1 Score (Not Available)', fontsize=15, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✓ Saved: {fig_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835f18a",
   "metadata": {},
   "source": [
    "## 2. Multiple Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b2103",
   "metadata": {},
   "source": [
    "**Note:** Change `MODEL_NAME` in the first cell to switch between models (baseline, gcn, gat, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e510af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "model_configs = [\n",
    "    ('baseline', 'BERT Baseline'),\n",
    "    ('gcn', 'GCN'),\n",
    "    ('gat', 'GAT'),\n",
    "    ('focal_loss', 'Focal Loss'),\n",
    "]\n",
    "\n",
    "# Use shared ablation folder for comparison plots\n",
    "comparison_dir = Path('../results/images/ablation')\n",
    "comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for model_name, label in model_configs:\n",
    "    history_path = models_dir / model_name / 'training_history.json'\n",
    "    if history_path.exists():\n",
    "        with open(history_path) as f:\n",
    "            history = json.load(f)\n",
    "        plt.plot(history['epochs'], history['train_loss'], marker='o', linewidth=2, label=label)\n",
    "    else:\n",
    "        print(f\"⚠️  {model_name}: training_history.json not found\")\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Model Comparison: Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(comparison_dir / 'model_comparison_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {comparison_dir / 'model_comparison_loss.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b92e38",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actual metrics from model checkpoints\n",
    "results = []\n",
    "model_configs = [\n",
    "    ('baseline', 'BERT Baseline'),\n",
    "    ('gcn', 'GCN'),\n",
    "    ('gat', 'GAT'),\n",
    "    ('focal_loss', 'Focal Loss'),\n",
    "    ('asymmetric_loss', 'Asymmetric Loss'),\n",
    "]\n",
    "\n",
    "for model_name, label in model_configs:\n",
    "    checkpoint_path = models_dir / model_name / 'best_model.pt'\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            # Extract metrics (adjust keys based on your checkpoint structure)\n",
    "            metrics = checkpoint.get('metrics', {})\n",
    "            \n",
    "            results.append({\n",
    "                'Model': label,\n",
    "                'Micro F1': metrics.get('micro_f1', 0.0),\n",
    "                'Macro F1': metrics.get('macro_f1', 0.0),\n",
    "                'Precision': metrics.get('precision', 0.0),\n",
    "                'Recall': metrics.get('recall', 0.0),\n",
    "                'Final Loss': checkpoint.get('loss', 0.0),\n",
    "                'Epoch': checkpoint.get('epoch', 0)\n",
    "            })\n",
    "            print(f\"✓ Loaded metrics for {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading {model_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️  {model_name}: checkpoint not found\")\n",
    "\n",
    "if results:\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"                MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = comparison_dir / 'ablation_results.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ Saved: {csv_path}\")\n",
    "else:\n",
    "    print(\"⚠️  No model results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867b92",
   "metadata": {},
   "source": [
    "## 4. Metrics Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8396c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "if results:\n",
    "    metrics = ['Micro F1', 'Macro F1', 'Precision', 'Recall']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(df)))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx]\n",
    "        bars = ax.bar(df['Model'], df[metric], alpha=0.8, edgecolor='black', linewidth=1.5, color=colors)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}',\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=25, labelsize=10)\n",
    "        ax.set_ylim(0, min(1.0, df[metric].max() * 1.2))\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparison_dir / 'metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved: {comparison_dir / 'metrics_comparison.png'}\")\n",
    "else:\n",
    "    print(\"⚠️  No results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab5acd",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix / Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions for current model\n",
    "pred_files = list(predictions_dir.glob(f'{MODEL_NAME}_*.pkl'))\n",
    "\n",
    "if pred_files:\n",
    "    # Load most recent prediction file\n",
    "    pred_file = sorted(pred_files)[-1]\n",
    "    print(f\"Loading: {pred_file.name}\")\n",
    "    \n",
    "    with open(pred_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    \n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Count labels per sample\n",
    "    label_counts = [len(p) for p in predictions]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(label_counts, bins=range(1, max(label_counts)+2), \n",
    "                 alpha=0.8, edgecolor='black', color='#F18F01')\n",
    "    axes[0].axvline(np.mean(label_counts), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Mean: {np.mean(label_counts):.2f}')\n",
    "    axes[0].set_xlabel('Number of Labels per Sample', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_title(f'{MODEL_NAME.upper()} - Label Distribution', fontsize=15, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot([label_counts], vert=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='#F18F01', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2),\n",
    "                    whiskerprops=dict(linewidth=1.5),\n",
    "                    capprops=dict(linewidth=1.5))\n",
    "    axes[1].set_ylabel('Number of Labels', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_title('Label Count Distribution', fontsize=15, fontweight='bold')\n",
    "    axes[1].set_xticklabels([MODEL_NAME.upper()])\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / 'label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved: {fig_dir / 'label_distribution.png'}\")\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Mean: {np.mean(label_counts):.2f}\")\n",
    "    print(f\"  Median: {np.median(label_counts):.1f}\")\n",
    "    print(f\"  Std: {np.std(label_counts):.2f}\")\n",
    "    print(f\"  Min/Max: {min(label_counts)} / {max(label_counts)}\")\n",
    "else:\n",
    "    print(f\"⚠️  No prediction files found for {MODEL_NAME} in {predictions_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d6a1",
   "metadata": {},
   "source": [
    "## 6. Top Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dde925",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_files:\n",
    "    # Count class frequencies\n",
    "    all_labels = [label for pred in predictions for label in pred]\n",
    "    label_freq = Counter(all_labels)\n",
    "    top_classes = label_freq.most_common(30)\n",
    "    \n",
    "    classes, counts = zip(*top_classes)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    bars = plt.barh(range(len(classes)), counts, alpha=0.8, edgecolor='black', \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(classes))))\n",
    "    plt.yticks(range(len(classes)), [f'Class {c}' for c in classes], fontsize=10)\n",
    "    plt.xlabel('Frequency', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('Class ID', fontsize=13, fontweight='bold')\n",
    "    plt.title(f'{MODEL_NAME.upper()} - Top 30 Predicted Classes', \n",
    "              fontsize=15, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        plt.text(count, i, f' {count}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / 'top_classes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved: {fig_dir / 'top_classes.png'}\")\n",
    "    print(f\"\\nTotal unique classes predicted: {len(label_freq)}\")\n",
    "    print(f\"Most frequent class: {classes[0]} ({counts[0]} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3940a06",
   "metadata": {},
   "source": [
    "## 7. t-SNE Visualization of Label Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to extract label embeddings\n",
    "checkpoint_path = models_dir / MODEL_NAME / 'best_model.pt'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    try:\n",
    "        print(f\"Loading model checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        # Extract classifier weights (label embeddings)\n",
    "        # Adjust key based on your model structure\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        # Find classifier layer\n",
    "        classifier_key = None\n",
    "        for key in state_dict.keys():\n",
    "            if 'classifier' in key.lower() and 'weight' in key:\n",
    "                classifier_key = key\n",
    "                break\n",
    "        \n",
    "        if classifier_key:\n",
    "            label_embeddings = state_dict[classifier_key].cpu().numpy()\n",
    "            print(f\"✓ Found label embeddings: {label_embeddings.shape}\")\n",
    "            \n",
    "            # Apply t-SNE\n",
    "            print(\"Running t-SNE... (this may take a minute)\")\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "            embeddings_2d = tsne.fit_transform(label_embeddings)\n",
    "            \n",
    "            # Visualize\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                                 c=range(len(embeddings_2d)), cmap='tab20',\n",
    "                                 alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            # Highlight some important classes\n",
    "            top_class_ids = [c for c, _ in top_classes[:10]] if pred_files else []\n",
    "            for cls_id in top_class_ids:\n",
    "                if cls_id < len(embeddings_2d):\n",
    "                    plt.scatter(embeddings_2d[cls_id, 0], embeddings_2d[cls_id, 1],\n",
    "                              c='red', s=200, marker='*', edgecolors='black', \n",
    "                              linewidth=1.5, label=f'Top Class {cls_id}' if cls_id == top_class_ids[0] else '')\n",
    "            \n",
    "            plt.colorbar(scatter, label='Class ID')\n",
    "            plt.xlabel('t-SNE Dimension 1', fontsize=13, fontweight='bold')\n",
    "            plt.ylabel('t-SNE Dimension 2', fontsize=13, fontweight='bold')\n",
    "            plt.title(f'{MODEL_NAME.upper()} - Label Embeddings (t-SNE)', \n",
    "                     fontsize=15, fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_dir / 'label_tsne.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✓ Saved: {fig_dir / 'label_tsne.png'}\")\n",
    "        else:\n",
    "            print(\"⚠️  Classifier layer not found in checkpoint\")\n",
    "            print(f\"   Available keys: {list(state_dict.keys())[:10]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"⚠️  Checkpoint not found: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95234272",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix for Hierarchical Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6290247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical level confusion (if you have true labels)\n",
    "# This requires validation/test labels\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "try:\n",
    "    from src.data_preprocessing import DataLoader\n",
    "    \n",
    "    # Load data to get hierarchy information\n",
    "    data_loader = DataLoader(data_dir='../data/raw/Amazon_products')\n",
    "    data_loader.load_all()\n",
    "    \n",
    "    # Load class hierarchy\n",
    "    hierarchy_file = Path('../data/raw/Amazon_products/class_hierarchy.txt')\n",
    "    \n",
    "    if hierarchy_file.exists():\n",
    "        # Parse hierarchy to get level information\n",
    "        class_levels = {}\n",
    "        \n",
    "        with open(hierarchy_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if '->' in line:\n",
    "                    parts = line.strip().split('->')\n",
    "                    if len(parts) == 2:\n",
    "                        parent, child = map(str.strip, parts)\n",
    "                        # Assign levels (simplified: count depth)\n",
    "                        class_levels[child] = class_levels.get(parent, 0) + 1\n",
    "        \n",
    "        if class_levels and pred_files:\n",
    "            # Count predictions by hierarchy level\n",
    "            level_counts = {}\n",
    "            \n",
    "            for pred in predictions:\n",
    "                for cls_id in pred:\n",
    "                    cls_name = str(cls_id)\n",
    "                    level = class_levels.get(cls_name, 0)\n",
    "                    level_counts[level] = level_counts.get(level, 0) + 1\n",
    "            \n",
    "            # Plot hierarchy level distribution\n",
    "            levels = sorted(level_counts.keys())\n",
    "            counts = [level_counts[l] for l in levels]\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            bars = plt.bar(levels, counts, alpha=0.8, edgecolor='black', \n",
    "                          color=plt.cm.Spectral(np.linspace(0, 1, len(levels))))\n",
    "            plt.xlabel('Hierarchy Level', fontsize=13, fontweight='bold')\n",
    "            plt.ylabel('Number of Predictions', fontsize=13, fontweight='bold')\n",
    "            plt.title(f'{MODEL_NAME.upper()} - Predictions by Hierarchy Level', \n",
    "                     fontsize=15, fontweight='bold')\n",
    "            plt.xticks(levels)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_dir / 'hierarchy_level_distribution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✓ Saved: {fig_dir / 'hierarchy_level_distribution.png'}\")\n",
    "        else:\n",
    "            print(\"⚠️  Hierarchy information not available\")\n",
    "    else:\n",
    "        print(f\"⚠️  Hierarchy file not found: {hierarchy_file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error analyzing hierarchy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5e6e2",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "**All visualizations have been saved!**\n",
    "\n",
    "### Model-Specific Images (`results/images/{MODEL_NAME}/`):\n",
    "- `training_curves.png`: Train/Val loss and F1 curves\n",
    "- `label_distribution.png`: Predicted label count distribution\n",
    "- `top_classes.png`: Top 30 most frequent predicted classes\n",
    "- `label_tsne.png`: t-SNE visualization of label embeddings\n",
    "- `hierarchy_level_distribution.png`: Predictions by hierarchy level\n",
    "\n",
    "### Cross-Model Comparisons (`results/images/ablation/`):\n",
    "- `model_comparison_loss.png`: Training loss comparison across models\n",
    "- `metrics_comparison.png`: F1, Precision, Recall comparison\n",
    "- `ablation_results.csv`: Detailed metrics table\n",
    "\n",
    "---\n",
    "\n",
    "### To visualize a different model:\n",
    "Change `MODEL_NAME` in the first cell to: `'baseline'`, `'gcn'`, `'gat'`, `'focal_loss'`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- Requires trained models in `models/{MODEL_NAME}/`\n",
    "- Requires predictions in `results/predictions/{MODEL_NAME}_*.pkl`\n",
    "- All images are saved at 300 DPI for publication quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
