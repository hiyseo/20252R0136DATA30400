# Hierarchical Multi-Label Classification Configuration

# Data
data:
  data_dir: "data/raw/Amazon_products"
  train_labels_path: "data/intermediate/train_silver_labels.pkl"
  test_labels_path: "data/intermediate/test_silver_labels.pkl"
  max_length: 128
  num_workers: 4

# Silver Labeling
silver_labeling:
  embedding_model: "sentence-transformers/all-mpnet-base-v2"
  use_keyword_matching: true
  use_semantic_similarity: true
  use_topdown_filtering: true  # Use hybrid top-down filtering
  keyword_weight: 0.3  # Weight for keyword matching score
  similarity_weight: 0.7  # Weight for semantic similarity score
  min_confidence: 0.1  # Minimum confidence threshold for final selection
  similarity_threshold: 0.5  # Minimum cosine similarity for semantic matching
  topdown_threshold: 0.15  # Threshold for top-down hierarchy filtering
  batch_size: 32  # Batch size for encoding
  
  # LLM keyword expansion (optional)
  llm_expansion:
    enabled: false  # Set to true to use LLM keyword expansion
    model: "gpt-4o-mini"  # or "gpt-3.5-turbo"
    max_calls: 1000  # Maximum API calls allowed
    min_keywords: 5  # Expand classes with fewer than this many keywords
    priority_limit: 100  # Number of priority classes to expand

# Model
model:
  model_name: "bert-base-uncased"  # or "roberta-base", "distilbert-base-uncased"
  model_type: "baseline"  # baseline, gcn, gat, focal_loss, etc.
  num_classes: 531
  dropout: 0.1
  freeze_encoder: false
  
  # For GNN models
  gnn_hidden_dim: 512
  gnn_num_layers: 2
  gnn_num_heads: 4  # For GAT only

# Self-Training (Baseline uses 2-stage training: BCE â†’ KLD)
self_training:
  enabled: true  # Baseline: true (BCE + Self-Training)
  confidence_threshold: 0.7
  max_iterations: 3
  min_improvement: 0.001
  epochs_per_iteration: 1

# Training
training:
  batch_size: 16
  num_epochs: 2  # Stage 1: BCE initialization (reduced to 2 for faster training)
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip_norm: 1.0
  
  # Loss function: bce (Stage 1), kld (Stage 2 - automatic in self-training)
  loss_type: "bce"
  
  # For focal loss
  focal_alpha: 0.25
  focal_gamma: 2.0
  
  # For hierarchical loss
  lambda_hier: 0.1

# Evaluation
evaluation:
  eval_every: 1  # Evaluate every N epochs
  save_every: 1  # Save checkpoint every N epochs
  metric: "micro_f1"  # Metric to optimize

# Output
output:
  output_dir: "models/{model_type}"  # Placeholder: will use model.model_type value
  predictions_dir: "results/predictions"
  images_dir: "results/images/{model_type}"
  log_dir: "logs"
  save_predictions: true

# Misc
misc:
  seed: 42
  device: "auto"  # auto, cpu, cuda, mps
  mixed_precision: true  # Use mixed precision training (faster on GPU)