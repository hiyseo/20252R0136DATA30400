# Hierarchical Multi-Label Classification Configuration

# Data
data:
  data_dir: "data/raw/Amazon_products"
  train_labels_path: "data/intermediate/train_silver_labels.pkl"
  test_labels_path: "data/intermediate/test_silver_labels.pkl"
  max_length: 128
  num_workers: 4

# Silver Labeling
silver_labeling:
  embedding_model: "sentence-transformers/all-mpnet-base-v2"
  use_keyword_matching: true
  use_semantic_similarity: true
  use_topdown_filtering: true  # Use hybrid top-down filtering
  keyword_weight: 0.3  # Weight for keyword matching score
  similarity_weight: 0.7  # Weight for semantic similarity score
  min_confidence: 0.5  # Increased from 0.1 to produce higher confidence labels
  similarity_threshold: 0.4  # Lowered from 0.5 to be more inclusive
  topdown_threshold: 0.10  # Lowered from 0.15 for better coverage
  batch_size: 32  # Batch size for encoding
  
  # LLM keyword expansion (optional)
  llm_expansion:
    enabled: false  # Set to true to use LLM keyword expansion
    model: "gpt-4o-mini"  # or "gpt-3.5-turbo"
    max_calls: 1000  # Maximum API calls allowed
    min_keywords: 5  # Expand classes with fewer than this many keywords
    priority_limit: 100  # Number of priority classes to expand

# Model
model:
  model_name: "bert-base-uncased"  # or "roberta-base", "distilbert-base-uncased"
  model_type: "baseline"  # baseline, gcn, gat, focal_loss, hierarchical_loss, etc.
  num_classes: 531
  dropout: 0.1
  freeze_encoder: false
  
  # For GNN models
  gnn_hidden_dim: 512
  gnn_num_layers: 2
  gnn_num_heads: 4  # For GAT only

# Self-Training (Baseline uses 2-stage training: BCE â†’ KLD)
self_training:
  enabled: true  # Re-enabled after confidence rescaling
  confidence_threshold: 0.5  # Increased from 0.3 (rescaled confidences are higher)
  max_iterations: 3
  min_improvement: 0.001
  epochs_per_iteration: 1

# Training
training:
  batch_size: 16
  num_epochs: 5  # Reduced to 5 since self-training is enabled
  learning_rate: 3.0e-5  # Slightly increased to prevent collapse
  weight_decay: 0.01  # Reduced to allow better learning
  warmup_ratio: 0.1
  gradient_clip_norm: 1.0
  
  # Loss function: bce (Stage 1), kld (Stage 2 - automatic in self-training)
  loss_type: "bce"  # 'focal', 'hierarchical', 'asymmetric'
  label_smoothing: 0.05  # Reduced from 0.1 (confidences now higher)
  
  # For focal loss
  focal_alpha: 0.25
  focal_gamma: 2.0
  
  # For hierarchical loss
  lambda_hier: 0.1

# Evaluation
evaluation:
  eval_every: 1  # Evaluate every N epochs
  save_every: 1  # Save checkpoint every N epochs
  metric: "micro_f1"  # Metric to optimize

# Output
output:
  output_dir: "data/models/{model_type}"  # Placeholder: will use model.model_type value
  predictions_dir: "results/predictions"
  training_dir: "results/training/{model_type}"  # Training visualization directory
  log_dir: "logs"
  save_predictions: true

# Submission
submission:
  student_id: "2020320135"  # Student ID for submission filename

# Misc
misc:
  seed: 42
  device: "auto"  # auto, cpu, cuda, mps
  mixed_precision: true  # Use mixed precision training (faster on GPU)