# Hierarchical Multi-Label Classification Configuration

# Data
data:
  data_dir: "data/raw/Amazon_products"
  train_labels_path: "data/intermediate/train_silver_labels.pkl"
  test_labels_path: "data/intermediate/test_silver_labels.pkl"
  max_length: 128
  num_workers: 4

# Silver Labeling
silver_labeling:
  embedding_model: "sentence-transformers/all-mpnet-base-v2"
  use_keyword_matching: true
  use_semantic_similarity: true
  keyword_weight: 0.3  # Weight for keyword matching score
  similarity_weight: 0.7  # Weight for semantic similarity score
  min_confidence: 0.1  # Minimum confidence threshold
  similarity_threshold: 0.5  # Minimum cosine similarity for semantic matching
  batch_size: 32  # Batch size for encoding

# Model
model:
  model_name: "bert-base-uncased"  # or "roberta-base", "distilbert-base-uncased"
  model_type: "bert"  # bert, gcn, gat
  num_classes: 531
  dropout: 0.1
  freeze_encoder: false
  
  # For GNN models
  gnn_hidden_dim: 512
  gnn_num_layers: 2
  gnn_num_heads: 4  # For GAT only

# Self-Training
self_training:
  enabled: false
  confidence_threshold: 0.7
  max_iterations: 3
  min_improvement: 0.001
  epochs_per_iteration: 1

# Training
training:
  batch_size: 16
  num_epochs: 5
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip_norm: 1.0
  
  # Loss function: bce, focal, asymmetric, hierarchical
  loss_type: "hierarchical"
  
  # For focal loss
  focal_alpha: 0.25
  focal_gamma: 2.0
  
  # For hierarchical loss
  lambda_hier: 0.1

# Evaluation
evaluation:
  eval_every: 1  # Evaluate every N epochs
  save_every: 1  # Save checkpoint every N epochs
  metric: "micro_f1"  # Metric to optimize

# Output
output:
  output_dir: "models/baseline"
  log_dir: "logs"
  save_predictions: true

# Misc
misc:
  seed: 42
  device: "auto"  # auto, cpu, cuda, mps
  mixed_precision: false  # Use mixed precision training (faster on GPU)